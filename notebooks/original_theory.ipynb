{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c16363",
   "metadata": {},
   "source": [
    "# DIP Side Project for Machine Learning\n",
    "\n",
    "Hi, my name is Josh. \n",
    "The purpose of this script was to enable my ML team to test our models with a cleaner dataset, and in turn optimize the time spent.\n",
    "\n",
    "While creating the script and learning + testing multiple libraries, I realized that this script may be of use to others. Please feel free to copy the script and optimize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca00182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import logging \n",
    "\n",
    "# Config log\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_local_time():\n",
    "    return datetime.now(ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "\n",
    "class SimpleTimer:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def elapsed(self):\n",
    "        seconds = time.time() - self.start_time\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.1f}s\"\n",
    "        elif seconds < 3600:\n",
    "            return f\"{seconds//60:.0f}m {seconds%60:.1f}s\"\n",
    "        else:\n",
    "            hours = seconds // 3600\n",
    "            minutes = (seconds % 3600) // 60\n",
    "            return f\"{hours:.0f}h {minutes:.0f}m\"\n",
    "    \n",
    "    def checkpoint(self, message):\n",
    "        elapsed_time = self.elapsed()\n",
    "        print(f\"{message} (Elapsed: {elapsed_time})\")\n",
    "        logging.info(f\"{message} (Elapsed: {elapsed_time})\")\n",
    "\n",
    "\n",
    "# Initialize timer\n",
    "timer = SimpleTimer()\n",
    "\n",
    "# Actual Configuration\n",
    "RESIZE_MODE = \"small\"\n",
    "BATCH_SIZE = 50  # Process images in batches to manage memory\n",
    "JPEG_QUALITY = 95  # (1-100)\n",
    "\n",
    "# AI helped me to discover the below edge case. \n",
    "# Configuration for input file extensions and grayscale handling\n",
    "INPUT_IMAGE_EXTENSIONS = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.dcm\"] # List of extensions to process\n",
    "CONVERT_GRAYSCALE_TO_RGB = True \n",
    "\n",
    "# Validate JPEG_QUALITY\n",
    "if not (1 <= JPEG_QUALITY <= 100):\n",
    "    raise ValueError(\"JPEG_QUALITY must be between 1 and 100.\")\n",
    "\n",
    "# These images are 512x512px, so this should be a 25% and 50% decrease in overall storage\n",
    "resize_settings = {\n",
    "    \"small\": (128, 128),\n",
    "    \"medium\": (256, 256)\n",
    "}\n",
    "\n",
    "resize_dim = resize_settings.get(RESIZE_MODE, (256, 256))\n",
    "# Validate RESIZE_MODE\n",
    "if RESIZE_MODE not in resize_settings:\n",
    "    logging.warning(f\"Warning: RESIZE_MODE '{RESIZE_MODE}' not recognized. Defaulting to {resize_dim}.\")\n",
    "\n",
    "\n",
    "dataset_root = Path(\"/kaggle/input/hnscc-zip/HNSCC_data\")\n",
    "ct_images_path = dataset_root / \"ct_images\"\n",
    "clinical_csv_path = dataset_root / \"clinical.csv\"\n",
    "\n",
    "# Output with resize information to allow for multiple runs of the script, by size.\n",
    "resize_suffix = f\"{resize_dim[0]}x{resize_dim[1]}\"\n",
    "output_root = Path(f\"/kaggle/working/Updated_Data_{resize_suffix}_{RESIZE_MODE}\")\n",
    "\n",
    "def print_table(data, headers, title=\"\", csv_path=None):\n",
    "    \"\"\"Print formatted table and optionally save to CSV\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    # Print table to console\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\", floatfmt=\".1f\"))\n",
    "    \n",
    "    \n",
    "    if csv_path:\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Table saved to: {csv_path}\")\n",
    "        logging.info(f\"Table '{title}' saved to: {csv_path}\")\n",
    "\n",
    "# Raise error instead of printing the error message from earlier versions.\n",
    "if not ct_images_path.exists():\n",
    "    logging.error(f\"The path {ct_images_path} does not exist.\")\n",
    "    raise FileNotFoundError(f\"The path {ct_images_path} does not exist.\")\n",
    "if not clinical_csv_path.exists():\n",
    "    logging.error(f\"The path {clinical_csv_path} does not exist.\")\n",
    "    raise FileNotFoundError(f\"The path {clinical_csv_path} does not exist.\")\n",
    "\n",
    "print(f\"Execution started at: {get_local_time().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "logging.infoprint(f\"Execution started at: {get_local_time().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output directory: {output_root}\")\n",
    "print(f\"Processing configuration: {RESIZE_MODE} mode, {resize_dim}, Quality: {JPEG_QUALITY}\")\n",
    "print(f\"Input image extensions: {', '.join(INPUT_IMAGE_EXTENSIONS)}\")\n",
    "print(f\"Convert grayscale to RGB: {CONVERT_GRAYSCALE_TO_RGB}\")\n",
    "\n",
    "\n",
    "# I updated the overall folder structure here to include the new logs.\n",
    "csv_to_folder_dir = output_root / \"csv_to_folder_check\"\n",
    "folder_to_csv_dir = output_root / \"folder_to_csv_check\"\n",
    "missing_data_dir = output_root / \"missing_data\"\n",
    "processing_logs_dir = output_root / \"processing_logs\"\n",
    "tables_dir = output_root / \"summary_tables\"\n",
    "visualizations_dir = output_root / \"visualizations\"\n",
    "error_logs_dir = output_root / \"error_logs\" \n",
    "\n",
    "for path in [csv_to_folder_dir, folder_to_csv_dir, missing_data_dir, processing_logs_dir, tables_dir, visualizations_dir, error_logs_dir]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# AI helped discover the need for an error log file, and its construction.\n",
    "error_log_file = error_logs_dir / \"image_processing_errors.log\"\n",
    "file_handler = logging.FileHandler(error_log_file)\n",
    "file_handler.setLevel(logging.ERROR)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(file_handler) # Add file handler to root logger\n",
    "\n",
    "print(\"Loading my dataset...\")\n",
    "logging.info(\"Loading my dataset...\")\n",
    "folder_ids = {p.name.replace(\"_ct_images\", \"\") for p in ct_images_path.iterdir() if p.is_dir()}\n",
    "df = pd.read_csv(clinical_csv_path)\n",
    "csv_ids = set(df[\"TCIA Radiomics dummy ID of To_Submit_Final\"].astype(str).str.strip())\n",
    "\n",
    "\n",
    "n_folders = len(folder_ids)\n",
    "n_csv_ids = len(csv_ids)\n",
    "\n",
    "timer.checkpoint(\"Data Loading Complete\")\n",
    "\n",
    "# CSV to Folder check\n",
    "print(f\"Performing data matching analysis...\")\n",
    "logging.info(\"Performing data matching analysis...\")\n",
    "csv_to_folder_results = []\n",
    "missing_csv_to_folder = []\n",
    "\n",
    "for idx, csv_id in enumerate(csv_ids, start=1):\n",
    "    match_found = csv_id in folder_ids\n",
    "    status = \"Confirmed\" if match_found else \"Missing\"\n",
    "    \n",
    "    csv_to_folder_results.append([idx, csv_id, match_found, status])\n",
    "    \n",
    "    if not match_found:\n",
    "        missing_csv_to_folder.append(csv_id)\n",
    "        logging.warning(f\"CSV ID '{csv_id}' has no corresponding image folder.\")\n",
    "\n",
    "\n",
    "# Folder to CSV check\n",
    "folder_to_csv_results = []\n",
    "missing_folder_to_csv = []\n",
    "\n",
    "for idx, folder_id in enumerate(folder_ids, start=1):\n",
    "    match_found = folder_id in csv_ids\n",
    "    status = \"Confirmed\" if match_found else \"Missing\"\n",
    "    \n",
    "    folder_to_csv_results.append([idx, folder_id, match_found, status])\n",
    "    \n",
    "    if not match_found:\n",
    "        missing_folder_to_csv.append(folder_id)\n",
    "        logging.warning(f\"Image folder '{folder_id}' has no corresponding CSV entry.\")\n",
    "\n",
    "\n",
    "# Print matching analysis tables\n",
    "print_table(\n",
    "    csv_to_folder_results[:10],  # This shows the first 10.\n",
    "    [\"Patient Number\", \"CSV ID\", \"Match Found\", \"Status\"],\n",
    "    \"CSV to Folder Matching Analysis (First 10)\",\n",
    "    tables_dir / \"csv_to_folder_analysis.csv\"\n",
    ")\n",
    "\n",
    "print_table(\n",
    "    folder_to_csv_results[:10], \n",
    "    [\"Folder Number\", \"Folder ID\", \"Match Found\", \"Status\"],\n",
    "    \"Folder to CSV Matching Analysis (First 10)\",\n",
    "    tables_dir / \"folder_to_csv_analysis.csv\"\n",
    ")\n",
    "\n",
    "# Simple Summary Table\n",
    "matched_count = len(folder_ids.intersection(csv_ids))\n",
    "match_rate = (matched_count / max(n_csv_ids, 1)) * 100\n",
    "\n",
    "data_quality_summary = [\n",
    "    [\"Total CSV Records\", n_csv_ids],\n",
    "    [\"Total Image Folders\", n_folders],\n",
    "    [\"Matched Pairs\", matched_count],\n",
    "    [\"CSV Missing Folders\", len(missing_csv_to_folder)],\n",
    "    [\"Folders Missing CSV\", len(missing_folder_to_csv)],\n",
    "    [\"Match Rate Percent\", f\"{match_rate:.1f}\"]\n",
    "]\n",
    "\n",
    "print_table(\n",
    "    data_quality_summary,\n",
    "    [\"Metric\", \"Count\"],\n",
    "    \"DATA QUALITY SUMMARY\",\n",
    "    tables_dir / \"data_quality_summary.csv\"\n",
    ")\n",
    "\n",
    "# Results\n",
    "pd.DataFrame(csv_to_folder_results, columns=[\"Patient Number\", \"CSV ID\", \"Match Found\", \"Status\"]).to_csv(csv_to_folder_dir / \"matched_and_unmatched.csv\", index=False)\n",
    "pd.DataFrame(folder_to_csv_results, columns=[\"Folder Number\", \"Folder ID\", \"Match Found\", \"Status\"]).to_csv(folder_to_csv_dir / \"matched_and_unmatched.csv\", index=False)\n",
    "pd.DataFrame({\"Missing CSV IDs (No Folder)\": missing_csv_to_folder}).to_csv(missing_data_dir / \"csv_without_folder_match.csv\", index=False)\n",
    "pd.DataFrame({\"Missing Folder IDs (No CSV)\": missing_folder_to_csv}).to_csv(missing_data_dir / \"folders_without_csv_match.csv\", index=False)\n",
    "\n",
    "\n",
    "matched_output_dir = output_root / f\"matched_folders_{resize_suffix}\"\n",
    "matched_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create list of matched patient IDs\n",
    "matched_ids = folder_ids.intersection(csv_ids)\n",
    "\n",
    "timer.checkpoint(\"Matching Analysis Complete\")\n",
    "\n",
    "# Initialize tracking dict\n",
    "processing_stats = defaultdict(lambda: {\n",
    "    'total_images': 0,\n",
    "    'successfully_processed': 0,\n",
    "    'skipped_images': [],\n",
    "    'error_messages': [],\n",
    "    'processing_time': 0,\n",
    "    'batch_count': 0\n",
    "})\n",
    "\n",
    "overall_stats = {\n",
    "    'total_images_found': 0,\n",
    "    'total_successfully_processed': 0,\n",
    "    'total_skipped': 0,\n",
    "    'patients_processed': 0,\n",
    "    'total_processing_time': 0\n",
    "}\n",
    "\n",
    "def process_image_batch(image_paths, dest_folder, patient_id):\n",
    "    \"\"\"Process a batch of images for memory efficiency\"\"\"\n",
    "    batch_start = time.time()\n",
    "    batch_stats = {'processed': 0, 'skipped': []}\n",
    "    \n",
    "    for image_file in image_paths:\n",
    "        try:\n",
    "            # Try block to check for grayscale first.\n",
    "            img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if img is None:\n",
    "                # AI helped me to find this edge case.\n",
    "                # If grayscale read fails, try reading as color.\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_UNCHANGED)\n",
    "                if img is None:\n",
    "                    error_msg = f\"Could not read image: {image_file.name}. File might be corrupted or unsupported.\"\n",
    "                    processing_stats[patient_id]['skipped_images'].append(image_file.name)\n",
    "                    processing_stats[patient_id]['error_messages'].append(error_msg)\n",
    "                    batch_stats['skipped'].append(image_file.name)\n",
    "                    logging.error(f\"Patient {patient_id}: {error_msg}\")\n",
    "                    continue\n",
    "\n",
    "            # Image resize\n",
    "            resized = cv2.resize(img, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Convert to RGB/BGR if specified and necessary.\n",
    "            if CONVERT_GRAYSCALE_TO_RGB:\n",
    "                if len(resized.shape) == 2: # It's grayscale.\n",
    "                    img_output = cv2.cvtColor(resized, cv2.COLOR_GRAY2BGR) # Conversion here\n",
    "                else: # Handles if image is already 3-channel.\n",
    "                    img_output = resized\n",
    "            else: # Do not force grayscale to RGB, save as is. \n",
    "                img_output = resized\n",
    "\n",
    "            # Save as configurable quality JPG/JPEG.\n",
    "            output_filename = image_file.stem + \".jpg\"\n",
    "            output_path = dest_folder / output_filename\n",
    "            cv2.imwrite(str(output_path), img_output, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])\n",
    "            \n",
    "            batch_stats['processed'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {image_file.name}: {str(e)}\"\n",
    "            processing_stats[patient_id]['skipped_images'].append(image_file.name)\n",
    "            processing_stats[patient_id]['error_messages'].append(error_msg)\n",
    "            batch_stats['skipped'].append(image_file.name)\n",
    "            logging.error(f\"Patient {patient_id}: {error_msg}\")\n",
    "    \n",
    "    # Waste Management (garbage collection) \n",
    "    gc.collect()\n",
    "    return batch_stats\n",
    "\n",
    "print(f\"\\nSTARTING IMAGE PROCESSING\")\n",
    "logging.info(\"STARTING IMAGE PROCESSING\")\n",
    "print(f\"Configuration: {RESIZE_MODE} {resize_dim} | JPEG Quality: {JPEG_QUALITY} | Batch Size: {BATCH_SIZE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "processing_start_time = time.time()\n",
    "\n",
    "# Process patients\n",
    "patient_summary_data = []\n",
    "\n",
    "for patient_id in tqdm(matched_ids, desc=\"Processing patients\"):\n",
    "    patient_start_time = time.time()\n",
    "    \n",
    "    src_folder = ct_images_path / f\"{patient_id}_ct_images\"\n",
    "    dest_folder = matched_output_dir / f\"{patient_id}_ct_images\"\n",
    "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files for this patient\n",
    "    image_files = []\n",
    "    for ext in INPUT_IMAGE_EXTENSIONS:\n",
    "        image_files.extend(list(src_folder.glob(ext)))\n",
    "    \n",
    "    processing_stats[patient_id]['total_images'] = len(image_files)\n",
    "    overall_stats['total_images_found'] += len(image_files)\n",
    "    \n",
    "    if not image_files:\n",
    "        logging.warning(f\"Patient {patient_id}: No image files found with extensions {INPUT_IMAGE_EXTENSIONS}.\")\n",
    "        continue\n",
    "    \n",
    "    # Process images in batches\n",
    "    batch_count = 0\n",
    "    for i in range(0, len(image_files), BATCH_SIZE):\n",
    "        batch = image_files[i:i + BATCH_SIZE]\n",
    "        batch_count += 1\n",
    "        \n",
    "        batch_results = process_image_batch(batch, dest_folder, patient_id)\n",
    "        processing_stats[patient_id]['successfully_processed'] += batch_results['processed']\n",
    "    \n",
    "    patient_processing_time = time.time() - patient_start_time\n",
    "    processing_stats[patient_id]['processing_time'] = patient_processing_time\n",
    "    processing_stats[patient_id]['batch_count'] = batch_count\n",
    "    \n",
    "    # Update overall stats\n",
    "    overall_stats['total_successfully_processed'] += processing_stats[patient_id]['successfully_processed']\n",
    "    overall_stats['total_skipped'] += len(processing_stats[patient_id]['skipped_images'])\n",
    "    overall_stats['patients_processed'] += 1\n",
    "    \n",
    "    # Collect data for summary table\n",
    "    success_rate = (processing_stats[patient_id]['successfully_processed'] / max(processing_stats[patient_id]['total_images'], 1)) * 100\n",
    "    \n",
    "    patient_summary_data.append([\n",
    "        patient_id,\n",
    "        processing_stats[patient_id]['total_images'],\n",
    "        processing_stats[patient_id]['successfully_processed'],\n",
    "        len(processing_stats[patient_id]['skipped_images']),\n",
    "        f\"{success_rate:.1f}\",\n",
    "        f\"{patient_processing_time:.2f}\",\n",
    "        batch_count\n",
    "    ])\n",
    "    logging.info(f\"Patient {patient_id} processed. Total images: {processing_stats[patient_id]['total_images']}, Processed: {processing_stats[patient_id]['successfully_processed']}, Skipped: {len(processing_stats[patient_id]['skipped_images'])}\")\n",
    "\n",
    "\n",
    "overall_stats['total_processing_time'] = time.time() - processing_start_time\n",
    "timer.checkpoint(\"Image Processing Complete\")\n",
    "\n",
    "# Patient Processing Summary Table\n",
    "print_table(\n",
    "    patient_summary_data[:15],  # Show first 15.\n",
    "    [\"Patient ID\", \"Total Images\", \"Processed\", \"Skipped\", \"Success Rate Percent\", \"Time Seconds\", \"Batches\"],\n",
    "    \"PATIENT PROCESSING SUMMARY (First 15)\",\n",
    "    tables_dir / \"patient_processing_summary.csv\"\n",
    ")\n",
    "\n",
    "# Overall Processing Statistics Table\n",
    "avg_time_per_patient = overall_stats['total_processing_time'] / max(overall_stats['patients_processed'], 1)\n",
    "images_per_second = overall_stats['total_successfully_processed'] / max(overall_stats['total_processing_time'], 1)\n",
    "overall_success_rate = (overall_stats['total_successfully_processed'] / max(overall_stats['total_images_found'], 1)) * 100\n",
    "\n",
    "processing_summary = [\n",
    "    [\"Patients Processed\", overall_stats['patients_processed']],\n",
    "    [\"Total Images Found\", overall_stats['total_images_found']],\n",
    "    [\"Successfully Processed\", overall_stats['total_successfully_processed']],\n",
    "    [\"Images Skipped\", overall_stats['total_skipped']],\n",
    "    [\"Overall Success Rate Percent\", f\"{overall_success_rate:.1f}\"],\n",
    "    [\"Total Processing Time Seconds\", f\"{overall_stats['total_processing_time']:.1f}\"],\n",
    "    [\"Average Time per Patient Seconds\", f\"{avg_time_per_patient:.2f}\"],\n",
    "    [\"Images per Second\", f\"{images_per_second:.1f}\"]\n",
    "]\n",
    "\n",
    "print_table(\n",
    "    processing_summary,\n",
    "    [\"Metric\", \"Value\"],\n",
    "    \"OVERALL PROCESSING STATISTICS\",\n",
    "    tables_dir / \"overall_processing_stats.csv\"\n",
    ")\n",
    "\n",
    "# Create Summary CSV\n",
    "image_count_summary = {\n",
    "    'Configuration': f\"{RESIZE_MODE}_{resize_suffix}\",\n",
    "    'Resize_Dimensions': f\"{resize_dim[0]}x{resize_dim[1]}\",\n",
    "    'JPEG_Quality': JPEG_QUALITY,\n",
    "    'Batch_Size': BATCH_SIZE,\n",
    "    'Input_Extensions': '; '.join(INPUT_IMAGE_EXTENSIONS),\n",
    "    'Convert_Grayscale_to_RGB': CONVERT_GRAYSCALE_TO_RGB,\n",
    "    'Total_Patients_Matched': len(matched_ids),\n",
    "    'Total_Images_Found': overall_stats['total_images_found'],\n",
    "    'Total_Images_Processed': overall_stats['total_successfully_processed'],\n",
    "    'Total_Images_Skipped': overall_stats['total_skipped'],\n",
    "    'Success_Rate_Percent': f\"{overall_success_rate:.1f}\",\n",
    "    'Processing_Time_Seconds': f\"{overall_stats['total_processing_time']:.1f}\",\n",
    "    'Total_Execution_Time': timer.elapsed(),\n",
    "    'Output_Directory': str(output_root),\n",
    "    'Processed_Images_Directory': str(matched_output_dir),\n",
    "    'Processing_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([image_count_summary])\n",
    "summary_csv_path = output_root / \"execution_summary.csv\"\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print_table(\n",
    "    [[k, v] for k, v in image_count_summary.items()],\n",
    "    [\"Configuration Item\", \"Value\"],\n",
    "    \"EXECUTION SUMMARY\",\n",
    "    summary_csv_path\n",
    ")\n",
    "\n",
    "# Create Visualizations\n",
    "timer.checkpoint(\"Creating performance visualizations\")\n",
    "\n",
    "# 1. Success Rate Distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "success_rates = [float(row[4]) for row in patient_summary_data]\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(success_rates, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Success Rates Across Patients')\n",
    "plt.xlabel('Success Rate (%)')\n",
    "plt.ylabel('Number of Patients')\n",
    "\n",
    "# 2. Processing Time vs Images\n",
    "plt.subplot(2, 2, 2)\n",
    "processing_times = [float(row[5]) for row in patient_summary_data]\n",
    "total_images = [int(row[1]) for row in patient_summary_data]\n",
    "plt.scatter(total_images, processing_times, alpha=0.6, color='green')\n",
    "plt.title('Processing Time vs Number of Images')\n",
    "plt.xlabel('Total Images per Patient')\n",
    "plt.ylabel('Processing Time (seconds)')\n",
    "\n",
    "# 3. Images per Patient Distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(total_images, bins=15, color='orange', alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Images per Patient')\n",
    "plt.xlabel('Number of Images')\n",
    "plt.ylabel('Number of Patients')\n",
    "\n",
    "# 4. Overall Statistics Pie Chart\n",
    "plt.subplot(2, 2, 4)\n",
    "labels = ['Successfully Processed', 'Skipped']\n",
    "sizes = [overall_stats['total_successfully_processed'], overall_stats['total_skipped']]\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Image Processing Results')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(visualizations_dir / \"processing_performance_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save all detailed reports\n",
    "processing_report = []\n",
    "for patient_id, stats in processing_stats.items():\n",
    "    processing_report.append({\n",
    "        'Patient_ID': patient_id,\n",
    "        'Total_Images': stats['total_images'],\n",
    "        'Successfully_Processed': stats['successfully_processed'],\n",
    "        'Skipped_Count': len(stats['skipped_images']),\n",
    "        'Processing_Time_Seconds': stats['processing_time'],\n",
    "        'Batch_Count': stats['batch_count'],\n",
    "        'Success_Rate_Percent': f\"{(stats['successfully_processed'] / max(stats['total_images'], 1)) * 100:.1f}\",\n",
    "        'Images_Per_Second': f\"{stats['successfully_processed'] / max(stats['processing_time'], 1):.2f}\",\n",
    "        'Skipped_Images': '; '.join(stats['skipped_images']) if stats['skipped_images'] else 'None',\n",
    "        'Error_Messages': '; '.join(stats['error_messages']) if stats['error_messages'] else 'None'\n",
    "    })\n",
    "\n",
    "processing_df = pd.DataFrame(processing_report)\n",
    "processing_df.to_csv(processing_logs_dir / \"detailed_processing_report.csv\", index=False)\n",
    "\n",
    "# Filter and save matched CSV\n",
    "matched_df = df[df[\"TCIA Radiomics dummy ID of To_Submit_Final\"].astype(str).str.strip().isin(matched_ids)]\n",
    "matched_df.to_csv(output_root / \"matched_clinical_data.csv\", index=False)\n",
    "\n",
    "# Final Summary Table\n",
    "final_summary = [\n",
    "    [\"Data Files\", \"\", \"\"],\n",
    "    [\"Matched Clinical CSV\", str(output_root / \"matched_clinical_data.csv\"), \"Complete\"],\n",
    "    [\"Resized Images Folder\", str(matched_output_dir), \"Complete\"],\n",
    "    [\"Analysis Tables\", \"\", \"\"],\n",
    "    [\"Data Quality Summary\", str(tables_dir / \"data_quality_summary.csv\"), \"Complete\"],\n",
    "    [\"Patient Processing Summary\", str(tables_dir / \"patient_processing_summary.csv\"), \"Complete\"],\n",
    "    [\"Execution Summary\", str(summary_csv_path), \"Complete\"],\n",
    "    [\"Visualizations\", \"\", \"\"],\n",
    "    [\"Performance Dashboard\", str(visualizations_dir / \"processing_performance_dashboard.png\"), \"Complete\"],\n",
    "    [\"Detailed Reports\", \"\", \"\"],\n",
    "    [\"Processing Log\", str(processing_logs_dir / \"detailed_processing_report.csv\"), \"Complete\"],\n",
    "    [\"Detailed Error Log\", str(error_log_file), \"Complete\"], # New: Error log entry\n",
    "    [\"All Output Files\", str(output_root), \"Complete\"]\n",
    "]\n",
    "\n",
    "print_table(\n",
    "    final_summary,\n",
    "    [\"Category\", \"Location\", \"Status\"],\n",
    "    \"FINAL OUTPUT SUMMARY\",\n",
    "    tables_dir / \"final_output_summary.csv\"\n",
    ")\n",
    "\n",
    "# Zip w/ resize info\n",
    "zip_filename = f\"Updated_Data_Archive_{resize_suffix}_{RESIZE_MODE}.zip\"\n",
    "zip_path = f\"/kaggle/working/{zip_filename}\"\n",
    "shutil.make_archive(base_name=zip_path.replace('.zip', ''), format='zip', root_dir=output_root)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProcessing complete! Total execution time: {timer.elapsed()}\")\n",
    "print()\n",
    "logging.info(f\"Processing complete! Total execution time: {timer.elapsed()}\")\n",
    "print(f\"Complete archive available at: {zip_path}\")\n",
    "print()\n",
    "print(f\"Check the '{tables_dir}' and '{visualizations_dir}' folders for detailed analysis.\")\n",
    "print()\n",
    "print(f\"Detailed error logs available at: {error_log_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64a4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
